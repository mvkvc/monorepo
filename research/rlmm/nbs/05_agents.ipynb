{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# agents\n",
    "\n",
    "> Reinforcement learning agent algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "from fastcore.test import *\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from abc import ABC\n",
    "from abc import abstractmethod\n",
    "from collections import deque\n",
    "import random\n",
    "from typing import *\n",
    "from typing import Tuple\n",
    "\n",
    "from fastcore.basics import patch\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"A simple replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, max_len: int):\n",
    "        \"\"\"Initialize the replay buffer.\"\"\"\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.memory: deque = deque(maxlen=self.max_len)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Return the length of the replay buffer.\"\"\"\n",
    "\n",
    "        return len(self.memory)\n",
    "\n",
    "    def push(self, transition: Tuple[np.ndarray, ...]):\n",
    "        \"\"\"Add a transition to the replay buffer.\"\"\"\n",
    "\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, batch_size: int):\n",
    "        \"\"\"Sample a batch of transitions from the replay buffer.\"\"\"\n",
    "\n",
    "        return random.sample(self.memory, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class BaseAgent(ABC):\n",
    "    @abstractmethod\n",
    "    def get_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(self, transition: Tuple[np.ndarray, ...]) -> float:\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def save(self, path) -> None:\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def load(self, path) -> None:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class RandomAgent(BaseAgent):\n",
    "    \"\"\"A simple random agent.\"\"\"\n",
    "\n",
    "    def __init__(self, action_size: Union[int, Tuple[float, float]]):\n",
    "        \"\"\"Initialize the agent.\"\"\"\n",
    "\n",
    "        self.action_size = action_size\n",
    "\n",
    "    def get_action(self, state: np.ndarray):\n",
    "        \"\"\"Get an action from the agent.\"\"\"\n",
    "\n",
    "        if isinstance(self.action_size, int):\n",
    "            return np.random.randint(self.action_size)\n",
    "        if isinstance(self.action_size, tuple):\n",
    "            return np.random.uniform(self.action_size[0], self.action_size[1])\n",
    "\n",
    "    def step(self, transition: Tuple[np.ndarray, ...]):\n",
    "        \"\"\"Take a step in the environment.\"\"\"\n",
    "\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class QNet(nn.Module):\n",
    "    \"\"\"A simple deep Q-network.\"\"\"\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_layers: List[int]):\n",
    "        \"\"\"Initialize the Q-network.\"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(input_size, hidden_layers[0])])\n",
    "        self.layers.extend(\n",
    "            [\n",
    "                nn.Linear(hidden_layers[i - 1], hidden_layers[i])\n",
    "                for i in range(1, len(hidden_layers))\n",
    "            ]\n",
    "        )\n",
    "        self.layers.append(nn.Linear(hidden_layers[-1], output_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the Q-network.\"\"\"\n",
    "\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "            x = torch.relu(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class DDQNAgent(BaseAgent):\n",
    "    \"\"\"A simple deep double Q-learning agent.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        state_size: int,\n",
    "        action_size: int,\n",
    "        hidden_layers: List[int],\n",
    "        lr: float,\n",
    "        epsilon: float,\n",
    "        epsilon_min: float,\n",
    "        epsilon_decay: float,\n",
    "        gamma: float,\n",
    "        update_freq: int,\n",
    "        target_copy_freq: int,\n",
    "        train_loops: int,\n",
    "        buffer_max: int,\n",
    "        buffer_min: int,\n",
    "        batch_size: int,\n",
    "    ):\n",
    "        \"\"\"Initialize the agent.\"\"\"\n",
    "\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.lr = lr\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.update_freq = update_freq\n",
    "        self.target_copy_freq = target_copy_freq\n",
    "        self.train_loops = train_loops\n",
    "        self.buffer_max = buffer_max\n",
    "        self.buffer_min = buffer_min\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.buffer = ReplayBuffer(max_len=self.buffer_max)\n",
    "\n",
    "        self.q = QNet(self.state_size, self.action_size, self.hidden_layers)\n",
    "        self.q_target = QNet(self.state_size, self.action_size, self.hidden_layers)\n",
    "        self.optimizer = optim.Adam(self.q.parameters(), lr=self.lr)\n",
    "        self.step_count = 0\n",
    "\n",
    "    def _update_target(self):\n",
    "        \"\"\"Update the target network.\"\"\"\n",
    "\n",
    "        self.q_target.load_state_dict(self.q.state_dict())\n",
    "\n",
    "    def _update_epsilon(self):\n",
    "        \"\"\"Update the epsilon value.\"\"\"\n",
    "\n",
    "        self.epsilon = max(\n",
    "            self.epsilon_min, self.epsilon * (1 - self.epsilon_decay) * self.step_count\n",
    "        )\n",
    "\n",
    "    def _update(self, transitions):\n",
    "        \"\"\"Update the Q-network.\"\"\"\n",
    "\n",
    "        state, action, reward, next_state, done_mask = transitions\n",
    "        state = torch.from_numpy(state).float()\n",
    "        action = torch.from_numpy(action).float()\n",
    "        reward = torch.from_numpy(reward).float()\n",
    "        next_state = torch.from_numpy(next_state).float()\n",
    "        done_mask = torch.from_numpy(done_mask).float()\n",
    "\n",
    "        q_out = self.q(state)\n",
    "        q_action = q_out.gather(1, action)\n",
    "        max_q_prime = self.q_target(next_state).max(1)[0].unsqueeze(1)\n",
    "        target = reward + self.gamma * max_q_prime * done_mask\n",
    "        loss = F.smooth_l1_loss(q_action, target)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"Get an action from the agent.\"\"\"\n",
    "\n",
    "        state = torch.from_numpy(state)\n",
    "        out = self.q(state).max(dim=1)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_size - 1)\n",
    "        else:\n",
    "            return out.argmax().item()\n",
    "\n",
    "    def step(self, env):\n",
    "        \"\"\"Take a step in the environment.\"\"\"\n",
    "\n",
    "        action = self.get_action(env.state)\n",
    "\n",
    "        state, action, reward, next_state, done, info = env.step(action)\n",
    "        done_mask = 1 if done else 0\n",
    "        transition = (\n",
    "            state,\n",
    "            np.array(action),\n",
    "            np.array(reward),\n",
    "            next_state,\n",
    "            np.array(done_mask),\n",
    "        )\n",
    "\n",
    "        self.buffer.push(transition)\n",
    "\n",
    "        if len(self.buffer) >= self.buffer_min:\n",
    "            for _ in range(self.train_loops):\n",
    "                transitions = self.buffer.sample(self.batch_size)\n",
    "                self._update(transitions)\n",
    "\n",
    "        if self.step_count % self.target_copy_freq == 0:\n",
    "            self._update_target()\n",
    "\n",
    "        self._update_epsilon()\n",
    "        self.step_count += 1\n",
    "\n",
    "        return done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | hide\n",
    "import nbdev\n",
    "\n",
    "nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

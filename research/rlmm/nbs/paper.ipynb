{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: thesis\n",
        "author:\n",
        "  - name: Marko Vukovic\n",
        "    email: marko.vukovic@torontomu.ca\n",
        "    affiliation:\n",
        "      - id: tmu\n",
        "        name: Toronto Metropolitan University\n",
        "        address: 350 Victoria Street\n",
        "        city: Toronto\n",
        "        state: Ontario\n",
        "        country: Canada\n",
        "        postal-code: M5B 2K3\n",
        "abstract: |\n",
        "  In this thesis, we aim to address the problem of developing a market making strategy that balances the goal of maximizing returns through spread trading while minimizing risk, particularly inventory risk. This involves training a neural network to act as a market making agent that can adapt to real market conditions through a data-driven approach. The proposed solution must consider constraints such as short selling restrictions and a minimum total value to avoid bankruptcy. The ultimate objective is to create a market making strategy that can effectively manage risk while achieving optimal returns.\n",
        "keywords:\n",
        "  - deep learning\n",
        "  - reinforcement learning\n",
        "  - market making\n",
        "date: last-modified\n",
        "bibliography: bibliography.bib\n",
        "lang: en-CA\n",
        "format:\n",
        "  pdf:\n",
        "    toc: true\n",
        "    keep-tex: true\n",
        "    number-sections: true\n",
        "    papersize: a4\n",
        "    fontsize: 12pt\n",
        "    linestretch: 1.5\n",
        "    margin-left: 1in\n",
        "    margin-right: 1in\n",
        "    margin-bottom: 1in\n",
        "    margin-top: 1in\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<!-- https://www.torontomu.ca/content/dam/graduate/student-guide/academic-matters/dissertation-thesis-exams/Thesis_MRP_Dissertation_Guidelines.pdf -->\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Motivation\n",
        "\n",
        "The field of deep reinforcement learning (DRL) has been experiencing a rapid growth in recent years, with a plethora of applications across diverse domains including robotics, gaming, and finance. In finance, market making is a crucial activity that involves balancing the supply and demand of securities to ensure their liquidity and stability. However, market making is a complex task that involves managing multiple constraints, such as risk management, regulatory requirements, and market conditions, to name a few.\n",
        "\n",
        "To address the challenge of market making, DRL has emerged as a promising approach, enabling the creation of autonomous agents that can adapt to changing market conditions and make decisions based on rewards and penalties. However, the conventional DRL models lack the capability of effectively incorporating constraints, resulting in suboptimal or even undesirable outcomes.\n",
        "\n",
        "The main aim of this thesis is to investigate the challenge of constraint handling in DRL-based market making. The thesis aims to overcome this challenge by incorporating constraints into the DRL model and developing novel algorithms that can optimize the market making strategies while taking into account these constraints. The proposed methods aim to improve the efficiency and robustness of the market making process, enabling the agent to operate within the constraints while maximizing its rewards. The ultimate goal of this research is to contribute to the advancement of DRL in market making and provide a framework for constraint-aware DRL-based market making.\n",
        "\n",
        "## Related work\n",
        "\n",
        "Previously, reinforcement learning has been applied to the task of market making. In a study by Spooner et al. (2018), a market simulator was developed to generate data for simulations based on historical data, and temporal difference methods were used along with other contributions to increase their effectiveness. This work was expanded upon by Spooner (2020), who created a similar market simulator and trained two agents simultaneously using the SARSA algorithm. One agent served as the market maker, while the other was a new adversarial agent that represented all other market participants, receiving a reward based on the losses of the market making agent. The market making agent trained in this manner was found to be more robust to adverse conditions. Other studies, such as Selser et al. (2021), have also applied deep reinforcement learning algorithms, specifically Q-learning, to the market making problem using their own market data simulator.\n",
        "\n",
        "To enhance the robustness of the market making agent, the field of safe or constrained reinforcement learning is explored. In Kou et al. (2020), a safety layer was added to the final prediction layer of the agent's neural network to ensure that the actions taken by the reinforcement learning agent do not exceed safety limits. Another approach, as described in Chow et al. (2018), is to use conditional value-at-risk as a percentile risk constraint instead of strict operational limits mapped to the action space. This is particularly relevant for applications in finance, as percentile metrics such as value at risk or conditional value at risk are commonly used for risk management.\n",
        "\n",
        "## Problem statement\n",
        "\n",
        "In this thesis, we aim to address the problem of developing a market making strategy that balances the goal of maximizing returns through spread trading while minimizing risk, particularly inventory risk. This involves training a neural network to act as a market making agent that can adapt to real market conditions through a data-driven approach. The proposed solution must consider constraints such as short selling restrictions and a minimum total value to avoid bankruptcy. The ultimate objective is to create a market making strategy that can effectively manage risk while achieving optimal returns.\n",
        "\n",
        "## Contribution\n",
        "\n",
        "# Background\n",
        "\n",
        "## Limit order book\n",
        "\n",
        "The limit order book is an important component of financial markets as it allows market participants to trade assets at the best available price, ensuring an efficient and fair market. For example, a trader looking to buy an asset can find the best available price by reviewing the sell limit orders in the order book and choosing the lowest price. This helps to ensure that all market participants have equal access to the best available prices for an asset, promoting a fair and transparent market.\n",
        "\n",
        "Each limit order (LO) must be placed at a specific price level that has been determined by the exchange and is known as the tick size. The tick size is a fixed amount, and the minimum time resolution of exchange market data is known as the tick level.\n",
        "\n",
        "When a limit order is placed, it can be fulfilled either fully or partially, depending on the volume of the order and the existing orders in the LOB. Orders placed at the current best price are known as market orders, and they are executed immediately as they take the current price. Market orders are different from limit orders as they do not have a specified price level and instead, they take the current price at the time of the execution.\n",
        "\n",
        "Limit orders are valid until they are cancelled or until their expiry date, which is usually set to 90 days from the date they were placed. When someone refers to the price of a stock, they usually mean the value at the midpoint between the prices for the highest bid and the lowest ask, also known as the market spread. For example, if the highest buy order is at the price level of $9.5 and the lowest sell order is at the price level of $10, then the market spread is $0.5.\n",
        "\n",
        "When a limit order is received by the exchange, it joins a queue at its specified price level, and it will be matched with an opposite order when all earlier orders at that price level are fulfilled, cancelled, or expire. If a limit order is submitted at a price level that has a matching order in the opposite direction, it will be instantly matched and filled by the exchange. For instance, if a buy order is placed for 100 shares at $10, and there are two sell orders at $10 for 50 and 75 shares, respectively, then the first sell order of 50 shares will be completely filled, and the second sell order will be partially filled for 50 out of the 75 shares, and the remaining 25 shares will remain in the LOB with first priority against later orders.\n",
        "\n",
        "A limit order book is a digital record of all outstanding limit orders for a particular financial asset. A limit order is a type of order placed by a trader that specifies the maximum price they are willing to pay (for a buy order) or the minimum price they are willing to receive (for a sell order) for a certain quantity of a financial asset. The limit order book collects all of these limit orders and displays them in a public and transparent manner, providing an overview of the supply and demand of the financial asset in question.\n",
        "\n",
        "Additionally, the limit order book also provides valuable information about market conditions and helps traders make informed decisions. By reviewing the supply and demand of an asset as represented in the limit order book, traders can gain insights into market trends and make decisions about their own trading strategies. This information can also be used by algorithmic traders to make high-frequency trading decisions based on real-time market data.\n",
        "\n",
        "In summary, the limit order book is a crucial component of financial markets as it promotes fairness, transparency, and efficiency. It provides market participants with access to the best available prices and valuable market information, enabling them to make informed trading decisions.\n",
        "\n",
        "## Market making\n",
        "\n",
        "Market making is a highly relevant financial trading strategy that is implemented to increase liquidity in a market by placing limit orders (LOs) at both sides of the limit order book (LOB). By doing so, market makers aim to profit from the spread of the transaction, which is the difference between the bid and ask price of a security. In other words, they buy a security at the bid price and sell it at the ask price, earning the spread as their profit.\n",
        "\n",
        "Market making can be performed independently by a trader, with incentives provided by the exchange, or even by the exchange itself. The benefits of market making are mutual for both the market maker and the exchange. The increased liquidity provided by the market maker improves exchange execution speed and price, benefiting the exchange, while the market maker profits from the spread of their transactions.\n",
        "\n",
        "It is common for exchanges to incentivize market making by offering a refund on a percentage of the market maker's transaction costs. This further encourages market makers to provide liquidity to the market, as they are able to offset some of their costs.\n",
        "\n",
        "Market making requires a high level of market knowledge, risk management skills, and quick execution speed to be successful. The market maker must be able to determine the best prices for their LOs and react quickly to changes in market conditions. In addition, they must be able to balance their positions and manage their risk effectively, as they are exposed to market volatility while holding securities in both directions.\n",
        "\n",
        "However, like all limit orders, the orders placed by market makers are not guaranteed to be fully filled at the specified price. As a result, market makers may accrue some non-zero inventory over the trading horizon. This inventory can generate significant losses due to price movements in the underlying asset, which is known as inventory risk. While inventory risk can result in profit, it is outside the core strategy of profiting from the spread and is therefore something that market makers aim to minimize.\n",
        "\n",
        "For example, if a market maker places a buy order for 10 shares at a price of $9.5 and a sell order for 10 shares at a price of $10.5, and the buy order is fully filled for 10 shares but the sell order is only half-filled for 5 shares, the market maker has received a profit of $5 from buying and selling 5 shares on each side with a spread of $1 each. However, they now also hold a long position of 5 shares at $10.5, with a portfolio value of $52.5. If the price of the stock drops by $2 during this period, the new value of the position is $42.5, resulting in a loss of $10 and a net loss of $5.\n",
        "\n",
        "Market making, while offering the potential for profit through the bid-ask spread, also entails various risks that must be acknowledged and carefully managed to minimize losses and optimize gains. Inventory risk is a prominent source of risk in market making and is a result of the buildup of holdings in the underlying asset due to unfulfilled or partially executed orders. If a market maker purchases more shares than they sell or vice versa, they will hold an inventory position that can result in monetary losses in the event of unfavorable price movements in the underlying asset. To reduce the impact of inventory risk, some market makers may pursue a net-zero inventory position at the conclusion of the trading day.\n",
        "\n",
        "The second source of risk is execution risk, which occurs when orders do not execute immediately or at the desired rate. For instance, if a market maker places a limit order to buy shares at a certain price, but the price moves higher before the order is executed, the market maker may end up buying the shares at a higher price than intended. Execution risk can also occur if orders on one side are executed at different rates than orders on the other side, leading to imbalanced positions and potentially unfavorable market conditions.\n",
        "\n",
        "The third source of risk is adverse selection risk, which arises from market conditions influencing order book behavior. For example, if there is an announcement of fraud at a company, this may lead to a decrease in buying activity, causing a market maker to accumulate an unfavorable inventory position. Adverse selection risk can also occur when market participants are aware that a market maker is looking to unwind a position and take advantage of this by lowering the prices they are willing to pay for the underlying asset.\n",
        "\n",
        "## Reinforcement learning\n",
        "\n",
        "Reinforcement Learning (RL) is a subfield of Artificial Intelligence (AI) and machine learning (ML) that focuses on developing algorithms that enable an agent to learn how to make decisions based on trial and error in an environment. The learning process involves the agent receiving rewards or penalties based on its actions, with the goal of maximizing cumulative reward over time. RL algorithms have found applications in a wide range of areas, including gaming, robotics, autonomous systems, and finance.\n",
        "\n",
        "In RL, the environment is modeled as a Markov Decision Process (MDP), where the state of the environment at each time step depends only on the previous state and the action taken by the agent. The agent interacts with the environment by selecting actions based on its current state, which then results in a new state and a reward. Over time, the agent learns to make optimal decisions based on its experience.\n",
        "\n",
        "RL algorithms can be classified into two categories: model-based and model-free. Model-based RL algorithms rely on an explicit representation of the environment's dynamics, while model-free algorithms do not require a model and instead learn directly from experience. Model-free algorithms can be further divided into value-based methods, where the agent learns a value function that estimates the expected future reward for each state, and policy-based methods, where the agent learns a policy that maps states to actions directly.\n",
        "\n",
        "RL has been applied to various financial applications, including algorithmic trading, portfolio management, and market making. By modeling financial markets as MDPs and training agents to optimize reward, RL has shown promising results in achieving improved trading performance compared to traditional methods.\n",
        "\n",
        "A Markov Decision Process (MDP) is a mathematical framework used in reinforcement learning to model an agent's decision making process. It consists of the following components:\n",
        "\n",
        "- A set of states $S$, where each state $s_i \\in S$ represents the current environment or situation of the agent.\n",
        "- A set of actions $A$, where each action $a_j \\in A$ represents a choice that the agent can make in each state.\n",
        "- A transition function $T(s, a, s')$, which describes the probability of transitioning from state $s$ to state $s'$ after taking action $a$. This function is defined as $T(s, a, s') = P(s_{t+1} = s' | s_t = s, a_t = a)$\n",
        "- A reward function $R(s, a, s')$, which defines the immediate reward received after transitioning from state $s$ to state $s'$ by taking action $a$.\n",
        "\n",
        "The goal of the agent in an MDP is to find an optimal policy $\\pi^*$ that maps each state to an action, such that the expected cumulative reward over a horizon of time is maximized. This can be represented mathematically as:\n",
        "\n",
        "$$\n",
        "\\pi^*(s) = \\arg\\max_{a \\in A} \\sum_{t=0}^{\\infty} \\gamma^t R(s_t, a_t, s_{t+1})\n",
        "$$\n",
        "\n",
        "where $\\gamma$ is the discount factor, which determines the weight given to future rewards.\n",
        "\n",
        "## Constrained reinforcement learning\n",
        "\n",
        "Constrained Markov Decision Processes (CMDPs) are an extension of traditional Markov Decision Processes (MDPs) that introduce constraints on the actions that an agent can take in a given state. A CMDP can be mathematically represented as a tuple $(S,A,P,R,\\gamma,C)$:\n",
        "\n",
        "  - $S$ is the set of states, representing the various situations in which the system can be.\n",
        "  - $A$ is the set of actions, representing the decisions that can be made in each state.\n",
        "  - $P_{s'|s,a}$ is the transition probability from state $s$ to state $s'$ given action $a$.\n",
        "  - $R_s^a$ is the reward obtained from taking action $a$ in state $s$.\n",
        "  - $\\gamma \\in [0,1]$ is the discount factor, representing the importance of future rewards.\n",
        "  - $C$ is a set of constraints, where each constraint is a function $f_i(s,a) \\le 0$ specifying a limit on the sum of rewards that can be received.\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\pi^*(s) &= \\arg\\max_{\\pi} \\sum_{t=0}^{\\infty} \\gamma^t r_{t+1} \\\\\n",
        "\\text{s.t.} & \\quad s_{t+1} = T(s_t, a_t, w_t) \\\\\n",
        "& \\quad a_t = \\pi(s_t) \\\\\n",
        "& \\quad b_t \\leq B\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "The goal of the CMDP is to find a policy $\\pi(s)$ that maps each state $s$ to an action $a$, so as to maximize the discounted sum of rewards subject to the constraints:\n",
        "\n",
        "Where $\\gamma \\in [0,1)$ is the discount factor, $r_{t+1}$ is the reward at time $t+1$, $T$ is the transition function that maps the current state and action to the next state, $w_t$ is a random disturbance, $a_t$ is the action taken at time $t$, $\\pi(s)$ is the policy that maps states to actions, $b_t$ is the constraint function, and $B$ is the upper bound on the constraints.\n",
        "\n",
        "The introduction of constraints allows for the representation of real-world situations where an agent's actions may be limited by external factors such as regulations or resource availability. In turn, this allows for more realistic and relevant modeling of decision-making scenarios, making CMDPs a useful tool for solving problems in fields such as finance, operations management, and control systems.\n",
        "\n",
        "The key challenge in solving a CMDP is finding a policy that maximizes the expected cumulative reward while respecting the constraints. This can be approached through various methods, including linear programming, dynamic programming, and reinforcement learning.\n",
        "\n",
        "## Reinforcement learning algorithms\n",
        "\n",
        "### Q-learning\n",
        "\n",
        "Q-learning is a reinforcement learning algorithm that is used to find the optimal action-value function for an agent in an environment. The Q-value for a state-action pair $(s, a)$ represents the expected cumulative reward that the agent would receive by taking action a in state s and following a fixed policy thereafter. The Q-learning algorithm updates the Q-values iteratively based on the observed rewards and the estimated Q-values of the subsequent states. The algorithm begins with an initial estimate for the Q-values, and then updates these estimates after each time step by using the following update rule:\n",
        "\n",
        "$$\n",
        "Q(s, a) = Q(s, a) + \\alpha (r + \\gamma \\max(Q(s', a')) - Q(s, a))\n",
        "$$\n",
        "\n",
        "where $\\alpha$ is the learning rate, r is the observed reward after taking action a in state s, $\\gamma$ is the discount factor, $s'$ is the next state after taking action a in state $s$, and $a'$ is the action with the highest estimated Q-value in state $s'$. The algorithm continues updating the Q-values until they converge to the optimal action-value function.\n",
        "\n",
        "The optimal action-value function represents the best possible expected cumulative reward that the agent can receive by taking actions optimally in each state. This function can be used to determine the optimal policy for the agent by selecting the action with the highest Q-value in each state. The Q-learning algorithm is a model-free algorithm, meaning that it does not require a model of the environment's transition dynamics. This makes the algorithm highly flexible and able to adapt to changes in the environment over time.\n",
        "\n",
        "Q-learning has been used in a variety of applications, including robotics, game playing, and decision making. Despite its simplicity, Q-learning has been shown to be highly effective in solving complex problems in these domains.\n",
        "\n",
        "### Deep Q-learning\n",
        "\n",
        "Deep Q-Learning is an advanced form of Q-Learning that leverages neural networks to approximate the Q-values for each action in a given state, instead of using a table as in regular Q-Learning. The neural network takes as input the state representation and outputs the Q-value for each action.\n",
        "\n",
        "The key difference between Deep Q-Learning and regular Q-Learning is that Deep Q-Learning can handle high dimensional state spaces, whereas regular Q-Learning requires a discretization of the state space. This discretization results in a potentially large and sparse Q-table, which can be computationally expensive to update and may not accurately capture the complex relationships between the state and action values.\n",
        "\n",
        "In Deep Q-Learning, the neural network acts as a function approximator, enabling the algorithm to learn and generalize the Q-values for a wide range of states, rather than being limited to the states it has seen in the past. The neural network can also capture non-linear relationships between the state and action values, which is difficult for traditional Q-Learning to do.\n",
        "\n",
        "To update the parameters of the neural network in Deep Q-Learning, the algorithm uses the Bellman equation to calculate the target Q-value for each action, and uses this target to train the neural network to minimize the difference between the predicted and target Q-values. This process is repeated until convergence, at which point the neural network will have learned an accurate approximation of the optimal Q-values for each state-action pair.\n",
        "\n",
        "### PPO\n",
        "\n",
        "Proximal Policy Optimization (PPO) is a reinforcement learning algorithm that is used to optimize the policy in a reinforcement learning problem. It combines elements from both value-based and policy-based methods and aims to strike a balance between exploration and exploitation in the optimization process.\n",
        "\n",
        "The algorithm uses a value function to estimate the expected reward for each action and updates the policy accordingly, so as to maximize the expected reward. The objective of the algorithm is to find a policy that maximizes the following expression:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\mathbb{E}_{t}\\left[\\min\\left(r_t(\\theta)\\hat{A}_t,\\text{clip}(r_t(\\theta),1-\\epsilon,1+\\epsilon)\\hat{A}_t\\right)\\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $\\theta$ represents the policy parameters\n",
        "- $r_t(\\theta)$ is the ratio of the new policy $\\pi_{\\theta}$ to the old policy $\\pi_{\\theta_{old}}$\n",
        "- $\\hat{A}_t$ is the advantage estimate for timestep $t$\n",
        "- $\\epsilon$ is a hyperparameter that defines the range for the clipping function $\\text{clip}$\n",
        "\n",
        "The algorithm updates the policy by taking gradient steps with respect to the objective function $J(\\theta)$ using a variant of stochastic gradient ascent. The clipping function ensures that the updated policy does not deviate too far from the previous policy, and helps to stabilize the optimization process.\n",
        "\n",
        "PPO is considered to be more sample-efficient and stable compared to other policy-based methods, and is widely used in deep reinforcement learning applications.\n",
        "\n",
        "# Methods\n",
        "\n",
        "## Data\n",
        "\n",
        "The dataset used for this study comprised of order book L2 data and executed trades on the Binance platform for various cryptocurrencies from the period of 6 months from March 2021 to September 2021 with sub-second granularity.\n",
        "\n",
        "The following table provides the relevant columns and data types for the order book data:\n",
        "\n",
        "| timestamp         | ask[0].price | ask[0].amount | bid[0].price | bid[0].amount | ... |\n",
        "|-------------------|--------------|---------------|--------------|---------------|-----|\n",
        "| **Data Type**     | float        | float         | float        | float         | ... |\n",
        "| **Description**   | Date & Time  | Ask price at level 0 | Ask amount at level 0 | Bid price at level 0 | Bid amount at level 0 | ...\n",
        "\n",
        "And here is the relevant columns and data types for the trades data:\n",
        "\n",
        "| timestamp         | kind  | price | amount |\n",
        "|-------------------|-------|-------|--------|\n",
        "| **Data Type**     | datetime | string | float  | float |\n",
        "| **Description**   | Date & Time | Type of order (Ask or Bid) | Trade price | Trade amount |\n",
        "\n",
        "## Experimental design\n",
        "\n",
        "### Training environment\n",
        "\n",
        "The training environment is a data driven simulator of a cryptocurrency exchange based on real market data provided by a third party vendor. Our approach is to train directly on historical tick level data from the order book. The various parts of our environment are as follows\n",
        "\n",
        "- $\\textit{State}$. State of the environment will be the complete set orders (price, volume) on the exchange as well the statistics of the agent. This includes thing such as available cash and inventory amounts at different price levels.\n",
        "- $\\textit{Actions}$. At every time step $t$ our agent with be able to submit $n$ tuples of both bid and ask limit orders $LO(P, N)$.\n",
        "- $\\textit{Rewards}$. The agent will receive a reward based on the total number of trades it completes as well as the profit it generates. Overall exchange operators are encouraged to have tight spreads in order to prevent their customers from going elsewhere or losing money due to arbitrage.\n",
        "\n",
        "State of the environment will be the complete LOB (price, volume) on the exchange as well other values about the underlying and the agent. This would include factors such as available cash and inventory amounts at different price levels. Exchanges operate on ticks for time $t$ meaning that we can formulate the objects as a discrete time MDP. As we know there will be some variable delay due to inference the time steps will be $t \\in t_n1, t_n1, ..., t_nk$ with $n1 ,n2, ..nk$ being the number of ticks it requires for the agent to process and submit an order.\n",
        "\n",
        "Along with discrete time increments every exchange has a minimum order price difference, causing the possible set of orders to be discrete as well. The action of placing a set of LO on either the buy or sell side can be represented as a sorted list similar to the definition of the LOB above.\n",
        "\n",
        "### Reward functions\n",
        "\n",
        "A common approach in reinforcement learning for finance is to set rewards for the agent with the sole aim of maximizing profits. However, in our case, we need to directly incentivize the agent to minimize risky behavior. To do this, we add a penalty to the reward function that discourages large changes in the trading position.\n",
        "\n",
        "At each time step $t_n$, the reward function can be formulated as follows:\n",
        "\n",
        "$$\n",
        "R_n = \\alpha \\cdot PL_n + \\beta \\cdot V_n - \\gamma \\cdot I_n - \\phi \\cdot \\sigma_{PL}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- $PL_n$ is the one-step realized profit or loss\n",
        "- $V_n$ is the current inventory value, represented by $Z_n(S_{n+1} - S_n)$ where $Z_n$ is the inventory level and $S_{n+1} - S_n$ is the change in price\n",
        "- $I_n$ is the change in inventory, represented by $\\Delta Z_n \\Delta S_n$\n",
        "- $\\alpha, \\beta, \\gamma$ are constants that represent the weighting of each of these factors and can be adjusted to fine-tune the reward function\n",
        "\n",
        "At the end of the trading day, an additional term is added to the reward function to account for the final time step. This is an inventory penalty term, represented by:\n",
        "\n",
        "$$\n",
        "R_{EOD} = R_{EOD-1} - \\rho \\cdot Z_{EOD}\n",
        "$$\n",
        "\n",
        "where $\\rho$ is a parameter that determines the scale of the penalty based on the amount of inventory remaining. The value of $\\rho$ can be adjusted to control the strength of the penalty for holding inventory at the end of the day.\n",
        "\n",
        "<!--  -->\n",
        "\n",
        "<!-- ## Problem statement\n",
        "\n",
        "For our environemnt, we will be training a model-free agent meaning that the agent does not learn a transition probability between states and actions. Any model-based approach as described will likely be too simplistic and is based on many factors which are unkown to the agent and likely to change constantly.\n",
        "\n",
        "The one step profit and loss $PnL$ is defined by the profit that the agent captures in the current timestep due to executing orders. This means that profit can be captured in three ways, from simultaneously executing a buy and sell order in the current time step, by placing an opposite sided order that captures the return from a previous order that is in the inventory, or by placing an order that fullfills a currently open order placed in the market. For example if there was an unfulfilled sell order and the agent buys a share to close the position the agent can earn a profit. To simplify the environment, we will book the profit from trading as a constant basis relative to the midprice and spread as follows\n",
        "\n",
        "$$\n",
        "    PnL = 100 * midprice * spread + 20 * midprice * 5 basis\n",
        "$$\n",
        "\n",
        "where $n \\in N$ is number of matched transactions at the time step, $V_n$ is the total number of shares at that price level, and $P_a$ and $P_b$ is the difference in price between the bid and ask orders. No short selling is allowed and when using shares from inventory the original book value of the share is used. Any orders not fullfilled in the current timestep are automatically considered to be cancelled, in order to simplify the training environment considering we are modelling a price taker with out data source.\n",
        "\n",
        "For this market making agent we are trying to solve an optimal control problem where the agent is trying to maximize the perfomance measure $J$ under the current policy $\\pi$\n",
        "\n",
        "$$\n",
        "    J(\\pi) = max E ( \\sum \\gamma R_n)\n",
        "$$\n",
        "\n",
        "which is the expectation of sum of future discounted rewards from our finite time horizon. In our setting, the time horizon can be at most one day, or if the agent runs below the minimum amount of money or violates any of the other constraints.\n",
        "\n",
        "We can represent optimal control problems as a Markov Decision Process (MDP) as defined previously and our agent has the goal of learning a stationary policy $\\pi: S \\rightarrow \\mathcal{P}(A)$ being a map from states to probability distributions over actions, with $\\pi(a \\mid s)$ denoting the probability of selecting action $a$ in state $s$. We denote the set of all stationary policies by $\\Pi$. -->\n",
        "<!--\n",
        "Then we aim to select a policy $\\pi \\in \\Pi$ which maximizes some performance measure, $J(\\pi)$, which is typically taken to be the infinite horizon discounted total return\n",
        "\n",
        "\\begin{equation}\n",
        "    J(\\pi) = \\underset{} {\\mathrm{E}} \\sum_{t=0}^{\\infty} \\gamma^{t} R - \\sigma R\n",
        "\\end{equation}\n",
        "\n",
        "where $\\gamma \\in[0,1)$ is the discount factor, and $R$ is the reward function at every time step that returns a value and the next state for every current state and given action.\n",
        "\n",
        "\\begin{equation}\n",
        "    J_{C_{i}}(\\pi) =\\underset{}{\\mathrm{E}}\\left[\\sum_{t=0}^{\\infty} \\gamma^{t} C_{i}\\left(s_{t}, a_{t},\n",
        "        s_{t+1}\\right)\\right]\n",
        "\\end{equation}\n",
        "\n",
        "The set of feasible stationary policies for a CMDP is then\n",
        "\n",
        "\\begin{equation}\n",
        "    \\Pi_{C} = \\left\\{\\pi \\in \\Pi: \\forall i, J_{C_{i}}(\\pi) \\leq d_{i}\\right\\}\n",
        "\\end{equation}\n",
        "\n",
        "and the reinforcement learning problem in a CMDP is\n",
        "\n",
        "\\begin{equation}\n",
        "    \\pi^{*}=\\arg \\max _{\\pi \\in \\Pi_{C}} J(\\pi)\n",
        "\\end{equation} -->\n",
        "\n",
        "<!-- Inventory risk $I_n$ is defined as holding a non-neutral position in a security, which in the context of market making can occur for a variety of market factors such as differing rates of order execution long vs short orders. As it is not in the core strategy of our market making agent we will look to reduce it wherever possible. For each day there is a $T_EOD$ value that will be given to the agent while training representing the time until the end of the current training day. At the end of the day the reward will be calculated with a penalty for keeping any overnight positions as shown\n",
        "\n",
        "$$\n",
        "    R_{EOD} = R_N - \\gamma I_N\n",
        "$$\n",
        "\n",
        "where $R_N$ is the reward at the last timestep before the end of the training day, and $\\gamma$ is a large penalty parameter that will penalize the agent for keeping overnight inventory. This will be in addition to the inventory penalty in the reward function which at every timestep gives a penalty to the reward based on the inventory.\n",
        "\n",
        "In the case of our environment and the constrained reinforcement learning problem the set of possible stationary policies are $\\Pi_{C} \\doteq\\left\\{\\pi \\in \\Pi: \\forall i, J_{C_{i}}(\\pi) \\leq d_{i}\\right\\}$ where $J_{C_i}$ is the performance measure including the cost functions associated with inventory risk and other hard limits such as having a positive cash amount which have an allowable range bounded by some predefined threshold $d_i$.\n",
        "\n",
        "Our agent needs to learn a policy $\\pi$ s.t. maximizes the total reward at each timestep based on the factors below\n",
        "\n",
        "For the reinforcement learning environment in our case we can set the relevant variable $S_t$ as the combination of different internal elements at every time step $t \\in T$.\n",
        "\n",
        "State: The state is represented as a combination of elements at each time step $S_t = \\{OB_t, I_t, w_t\\}$. This includes the current order book $OB_t$, additional indicators $I_t$, and the weights of the NN used by the agent $w_t$. This set contains the complete set of factors that the agent utilizes to compute actions at each time step $t$.\n",
        "\n",
        "Order book: We can represent the order book as a grid of values where each column represents a price level and the rows represent individual limit orders which are arranged in priority order of execution. We can write this as an ordered set of values $OB_t = [\\{A_{-N}, P_{-N}\\}, ..., \\{A_{-1}, P_{-1}\\}, \\{A_{0}, P_{0}\\}, \\{A_{1}, P_{1}\\}, ..., \\{A_{N}, P_{N}\\}]$ where $N$ is the maximum depth of the order book that our agent can view. Initially we reviewed the top 5 price levels on both sides of the order book for a total of 10 total price levels.\n",
        "\n",
        "Internal variables: $I_t = [x_t, I_t, EOD_t]$ where each value is an internal measure from the environment including $x_t $the agent's cash level, $I_t$ inventory level, $EOD_t$ time until the end of the trading day.\n",
        "\n",
        "The current inventory can be represented in the same way as the order book where it is an ordered set of price, amount pairs $P_{LO}$, $A_{LO}$ that record the book price of the shares purchased. We cannot hold any short positions in inventory and these can only be closed by placing sell orders.\n",
        "\n",
        "Actions: $A_t = [A_{-N}, ..., A_{-1}, A_0, A_1, ..., A_N]$ where each value $A_n$ represent a limit order placed at an existing price level $n \\in [-N, N]$ levels from the midpoint and amount $An$ where a negative amount confers a sell order.\n",
        "\n",
        "Rewards: The reward function is a weighted combination of variables include the profit and loss, the trading volume at each time step, the inventory level, and the standard deviation of returns defined earlier as $R_n$. -->\n",
        "\n",
        "<!-- ## Data\n",
        "\n",
        "WIP\n",
        "\n",
        "\n",
        "## Training method\n",
        "\n",
        "WIP\n",
        "\n",
        "# Results\n",
        "\n",
        "WIP\n",
        "\n",
        "# Conclusions\n",
        "\n",
        "WIP\n",
        "\n",
        "# Notes\n",
        "\n",
        "## Background\n",
        "\n",
        "The process that we are attempting to model in this case is a market making agent in financial markets. These entities are required to continuosly quote an ask and bid price throughout the day at which they take orders. The market maker makes a profit based on the spread between the ask and bid orders executed and the total volume executed. The potential losses come from directional risk where only sell orders are being executed when the price drops, as well as holding excess inventory when the price drops. The goal for this paper is to develop a reinforcement learning system that can quote prices and amounts for the agent to maxmize profits while also containing a component to manage inventory risk.\n",
        "\n",
        "<!-- Market making described clear and target what are they trying to achieve and what kind of risk amnagement are they trying to contain. Few things to consider etc 1. 2. 3. make a point\n",
        "\n",
        "Constraints mapping 1 to 1 from the constraints to the background to the constraints section expand in more detaill -->\n",
        "\n",
        "<!-- ## Environment structure -->\n",
        "\n",
        "<!-- Rename to model\n",
        "Introduce all the variables\n",
        "ETC\n",
        "Table with all the variables\n",
        "Once everything is in mathematics create a simple example\n",
        "Contribution section\n",
        "Math modelling section incl table with all the terms\n",
        "Remember when you introduce the notation -->\n",
        "\n",
        "<!-- Computing environment description -->\n",
        "\n",
        "<!-- The training environment for our agents is a data driven simplified market simulator. Given the historical order book and trades data, we construct an initial position for the agent. With this initial position the agent selects a price level for two seperate bid and ask orders, along with desired amount to place those orders for. These orders will be placed as limit orders in the market and can be configured to cancel after a defined $N$ steps of the environment.\n",
        "\n",
        "In order to determine if the orders are filled or not we calculate the change between current and next timestep for the order book and trades data. The two distinct cases for this calculation are as follows taken from the context of a specific price level.\n",
        "\n",
        "- $\\Delta_{\\textnormal{order book}} > \\Delta_{\\textnormal{trades}}$ : In this case change in order book volume is larger than the change in the trades and therefore we determine that $\\Delta_{\\textnormal{order book}} = \\Delta_{\\textnormal{trades}} + \\textnormal{new orders}$ where the difference is due to new orders coming in.\n",
        "- $\\Delta_{\\textnormal{order book}} < \\Delta_{\\textnormal{trades}}$ : In this case change in order book volume is less than the change in the trades and therefore we determine that $\\Delta_{\\textnormal{order book}} = \\Delta_{\\textnormal{trades}} + \\textnormal{new orders} - \\textnormal{cancelled orders}$ where new orders are coming in and the negative diff between $\\Delta_{\\textnormal{order book}}$ and $\\Delta_{\\textnormal{trades}}$ is due to order cancellations (which will be uniformly distributed around the agents existing orders).\n",
        "\n",
        "The state of the environment which is fed to the agent will be as follows.\n",
        "\n",
        "- Vector of last $N$ order book volumes at price levels relative to current midpoint\n",
        "- Vector of current existing order volumes at price levels relative to the current midpoint\n",
        "- Vector of internal environment statistics such as the amount of cash and inventory available\n",
        "\n",
        "The action set for the agent will be as follows and is different for a discrete and open formulation.\n",
        "\n",
        "- Discrete: Enumerate all combinations of bid and ask price levels for the top $N$ levels for each. Ex. N=5 price levels becomes 25 combinations of price levels. This can be reduced by hand selecting strategic combinations which has been done in related works. The amount trading will be an environment parameter and can only be changed by the constraint module which will be discussed further on.\n",
        "- Open: Take the price levels as indivudal integers from 0 to the maximum price level selected for the environment up to 25 for both the bid and ask orders. Take the amount values as individual floats between 0 and 1. For the bid amount the float represents the relative amount of total cash to pledge, and for ask amounts it represents the relative amount of total inventory to pledge.\n",
        "\n",
        "## Constraints\n",
        "\n",
        "For our agent we want to add a constraint module that will lower our inventory risk to some environment specified amount. For this criteria we have chosen the conditial value at risk ($CVaR$) risk measure as it is a commonly used measure in industry. It is defined as follows.\n",
        "$$\n",
        "VaR_{\\alpha} = \\inf \\{x:P(L>x)\\leq(1-\\alpha)\\}\n",
        "$$\n",
        "\n",
        "$$\n",
        "CVaR_{\\alpha}(X) = \\frac{1}{\\alpha}\\int_{0}^{\\alpha}VaR_{s}(X)ds\n",
        "$$\n",
        "\n",
        "The way that this agent can influence this risk measure is directly by the amount of inventory they hold. As the agent is always forming a portfolio with two assets, cash and the underlying assset of the market it can reduce it's $CVaR$ by reducing inventory.\n",
        "\n",
        "As the calculation of $VaR$ and $CVaR$ are expensive to calculate in real time, we calulate VaR as a daily figure ahead of time on the historical data. Then for any configuration of $CVaR$ the value can be translated to a soft inventory target for the constraint module to optimize.\n",
        "\n",
        "## Neural networks\n",
        "\n",
        "The are two possible types of neural networks which will be used in our formulation.\n",
        "\n",
        "- FC: fully connected network aka multi layer perceptron\n",
        "  - All agent output modules and constraint modules\n",
        "- RNN: recurrent neurral network\n",
        "  - Potentially used as base learner in open formulation\n",
        "\n",
        "They will be arranged as follows in both formulation cases.\n",
        "\n",
        "- Discrete -->\n",
        "\n",
        "<!-- ![discrete network](assets/discrete_network.excalidraw.png){fig.pos='H' width=300} -->\n",
        "<!--  -->\n",
        "\n",
        "<!-- - Open -->\n",
        "\n",
        "<!-- ![open network](assets/open_network.excalidraw.png){fig.pos='H' width=500} -->\n",
        "<!--  -->\n",
        "\n",
        "<!-- ## Reward functions\n",
        "\n",
        "- Reward maximizing modules\n",
        "\t- returns from trading + k * variance(returns from trading)\n",
        "- Constraint modules\n",
        "  - -change in actions + k * (distance from inventory to target level)^2\n",
        "\n",
        "## Reinforcement learning algorithms\n",
        "\n",
        "The choice of reinforcement learning algorithms will be as follows.\n",
        "\n",
        "- Discrete\n",
        "  - Variant of DQN with fixed actions that are perturbed by constraint module\n",
        "- Open\n",
        "  - Exact algorithm not yet determined\n",
        "\n",
        "## Training strategy\n",
        "\n",
        "Randomly sample a training day, training time start point (min time length according to env), and starting parameters ex. cash and inventory. Run the environment until the end of the trading day or the value of the agents position drops below the minimum. --> -->\n",
        "\n",
        " <!-- In this problem we have a reinforcement learning agent acting as a market maker for an underlying asset. We utilize\n",
        "     historical order book data represented as the top 25 price and amount bids and asks at each time period. This\n",
        "     order book snapshot at every time step $t$ can be represented as a set $O_{t}$ of price, amount tuples\n",
        "     $\\{\\{P_n, A_n\\}, \\{P_{n+1}, A_{n+1}\\}, ..., \\{P_{25}, A_{25}\\}\\}$. Additionally we have a list of quantitative\n",
        "     indicators relating to the asset represented as a vector ${I_t}$. In the market simulator we construct, at each\n",
        "     step $t$ the agent will receive $O_{t}$ and ${I_t}$ as inputs and return a set of tuples representing the\n",
        "     actions to take in this timestep $A_t$ consisting of price, amount tuples up to $\\{P_n, A_n\\}, \\{P_{n+1},\n",
        "     A_{n+1}\\}, ..., \\{P_{N}, A_{N}\\}$ with $N$ being an input parameter on the maximum possible number of actions\n",
        "     to take in each time step. This set of actions will be returned to the simulator which will calculate the new\n",
        "     position and cash amount at each time step. The simulator is also calculating an internal risk management\n",
        "     measure $C$ that is a constraint on the potential actions of the agent. At the end of each trading day the\n",
        "     agent must be in the acceptable range of the measure as denoted by $C_\\text{min}$ and $C_\\text{max}$. The\n",
        "     amount of starting cash is a tuneable parameter and the simulation ends when the time period $t, t+1, ..., T$\n",
        "     has elapsed, the agent runs out of cash, or ends the trading day with an unacceptable value in the risk measure\n",
        "     . As described in review paper by Garcia \\cite{garcia15a} the expectation of the return is maximized subject to\n",
        "     one or more constraints, $c_{i} \\in C$. The general form of this criterion is shown in the following\n",
        "\n",
        " We will need a financial risk measure in order to constrain our agent s.t. it does not take on excess risk.\n",
        " Conditional value-at-risk or CVaR is a common risk management metric used in the industry, with many advantages\n",
        " over the traditional value-at-risk or VaR metric. CVaR has superior mathematical properties versus VaR such as\n",
        " being classified as a coherent risk measure.\n",
        "\n",
        " \\\n",
        "\n",
        " For random variables with continuous distribution functions, CVaR $_{\\alpha}(X)$ equals the conditional\n",
        " expectation of $X$ subject to $X \\geq \\operatorname{VaR}_{\\alpha}(X)$. The CVaR of $X$ with confidence level\n",
        " $\\alpha \\in] 0,1[$ is the mean of the generalized $\\alpha$-tail distribution:\n",
        "\n",
        " $$\n",
        " \\operatorname{CVaR}_{\\alpha}(X)=\\int_{-\\infty}^{\\infty} z d F_{X}^{\\alpha}(z),\n",
        " $$\n",
        "\n",
        " where\n",
        "\n",
        " $$\n",
        " F_{X}^{\\alpha}(z)= \\begin{cases}\n",
        "                        0, & \\text { when } z<\\operatorname{VaR}_{\\alpha}(X), \\\\ \\frac{F_{X}(z)-\\alpha}{1-\\alpha},\n",
        "                        & \\text { when } z \\geq \\operatorname{VaR}_{\\alpha}(X) .\n",
        " \\end{cases}\n",
        " $$\n",
        "\n",
        " \\\n",
        "\n",
        " Write the problem statement based on what is in Section 1 and Section 2 how can we build out this mathematical\n",
        "     model. Translate the text from the beginning sections into a mathematical optimization problem based on what we\n",
        "     have written. Optimization problem, the constraints, etc. Reinforcement learning textbook or papers and it\n",
        "     should match the nomenclature eg. create a mathemtical optimization from our text statemetn whcih can be shown\n",
        "     to be represented as an RL problem, which we can show to use deep learning as well etc.\n",
        "\n",
        " Make it into one table etc. using spark and h5 and different exchanges cahnge the timestamp into the real time.\n",
        "     Hour minute second etc. Need to write a description regarding the data and the structure for the data section.\n",
        "\n",
        " where ci represents the ith constraint in $C$ that the policy $\\pi$ must fulfill, with $c_{i}=\\left\\{h_{i} \\leq\n",
        "     \\alpha_{i}\\right\\}$ where $h_{i}$ is a function related with the return and $\\alpha_{i}$ is the threshold\n",
        "     restricting the values of this function. Depending of the problem the symbol $\\leq$ in the constraints $c_{i}\n",
        "     \\in C$ may be replaced by $\\geq$. -->\n",
        "\n",
        "# References {-}\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}